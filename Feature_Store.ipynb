{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmiWKUPXQAza"
      },
      "source": [
        "# Implementing a Feature Store to predict loan eligibility\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1dwtH_4Qbe5"
      },
      "source": [
        "## Feature Store Explainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzOSuB-P-rz6"
      },
      "source": [
        "### What is a feature store?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY3F6xSt-vDu"
      },
      "source": [
        "Before we dive into what a feature store is, quick refresher: in machine learning, a feature is data used as input in a predictive model. It is the x in f(x) = y\n",
        "\n",
        "A feature store is an ML-specific system that:\n",
        "\n",
        "- Transform raw data into feature values for use by ML models - think a data pipeline\n",
        "- Stores and manages this feature data, and\n",
        "- Serves feature data consistently for training and inference purposes\n",
        "\n",
        "#### What problem are feature stores trying to solve?\n",
        "\n",
        "Feature stores are trying to solve 3 problems:\n",
        "\n",
        "* When an ML model is trained on preprocessed data, it is necessary to carry out the identical steps on incoming prediction requests. This is because we need to provide the model data with the same characteristics as the data it was trained on. If we don窶冲 do that, we will get a difference between training and serving, and the model predictions will not be as good.\n",
        "* Many companies will use the same features across a variety of models and so it is a central hub for those features to be used by many models. Feature stores make sure there is no repetitive engineering setup as well as different pre-processing steps for the same features\n",
        "* It takes care of the engineering burden making sure features are pre-loaded into low-latency storage without the engineering work as well as making sure that these features were calculated the same way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbjxAykZwveS"
      },
      "source": [
        "### When to use a feature store\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4l98iVuJcgE"
      },
      "source": [
        "In most cases feature stores add unnecessary complexity and are well suited for specific ML uses cases. You might even be asking, \"If a feature store is simply making sure the same pre-processing happens on the data, why can't I do that transformation during inference on the raw data?\"\n",
        "\n",
        "There are two scenarios that it isn't viable:\n",
        "\n",
        "\n",
        "*   The first situation is if the feature value will not be known by clients requesting predictions, but has to instead be computed on the server. If the clients requesting predictions will not know the feature values, then we need a mechanism to inject the feature values into incoming prediction requests. The feature store plays that role. For example, one of the features of a dynamic pricing model may be the number of web site visitors to the item listing over the past hour. The client (think of a mobile app) requesting the price of a hotel will not know this feature窶冱 value. This information has to be computed on the server using a streaming pipeline on clickstream data and inserted into the feature store. You can also imagine that if you have to fetch a alot of data, this cannot be done quick enough.\n",
        "\n",
        "* The second situation is to prevent unnecessary copies of the data. For example, consider that you have a feature that is computationally expensive and is used in multiple ML models. Rather than using a transform function and storing the transformed feature in multiple ML training datasets, it is much more efficient and maintainable to store it in a centralized repository.\n",
        "\n",
        "To summarize, a feature-store is most valuable when:\n",
        "\n",
        "* A feature is unknown by user and needs to be fetched/computed server-side\n",
        "* A feature requires intensive computation\n",
        "* A Feature is used by many different models\n",
        "\n",
        "Okay, thats enough english for now. Lets get to the building since I prefer to speak in code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXb8YqI0JsUN"
      },
      "source": [
        "## Tutorial\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYFHH6UNQSYp"
      },
      "source": [
        "Throughout this tutorial, we窶冤l walk through the creation of a production-ready fraud prediction system, end to end. We will be predicting whether a transaction made by a given user will be fraudulent. This prediction will be made in real-time as the user makes the transaction, so we need to be able to generate a prediction at low latency.\n",
        "\n",
        "Our system will perform the following workflows:\n",
        "- Computing and backfilling feature data from raw data\n",
        "- Building point-in-time correct training datasets from feature data and training a model\n",
        "- Making online predictions from feature data\n",
        "\n",
        "We will be using a open-source framework called Feast which is built from the guys a Tecton, one of the leading feature-store companies globally. Tecton is a hosted version of Feast and comes with a few more beneficial features such as monitoring. We will then be deploying our application to AWS.\n",
        "\n",
        "If you don't have it, download the data required for this tutorial from [here](https://drive.google.com/file/d/1MidRYkLdAV-i0qytvsflIcKitK4atiAd/view?usp=sharing). This is originally from a [Kaggle dataset](https://www.kaggle.com/competitions/ieee-fraud-detection/data) for Fraud Detection. Place this dataset in a `data` directory in the root of your project. You can run this notebook either in VS Code or Jupyter Notebooks.\n",
        "\n",
        "We're going to convert this dataset into a format that Feast can understand, a parquet file. We also need to add 2 columns, `event_timestamp` and `created_timestamp`, so that feast can index the data time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "df = pd.read_csv('data/train_transaction.csv')\n",
        "df[\"TransactionDT\"] = df[\"TransactionDT\"]/df[\"TransactionDT\"].max()\n",
        "\n",
        "start = datetime(2021, 1, 1).timestamp()\n",
        "end = datetime(2022, 1, 1).timestamp()\n",
        "\n",
        "df[\"event_timestamp\"] = pd.to_datetime(df[\"TransactionDT\"].apply(lambda x: round(start + x * (end - start))), unit='s')\n",
        "df[\"created_timestamp\"] = df[\"event_timestamp\"].copy()\n",
        "\n",
        "df = df[[\"ProductCD\", \"TransactionAmt\", \"P_emaildomain\", \"R_emaildomain\", \"card4\", \"M1\", \"M2\", \"M3\", \"created_timestamp\", \"event_timestamp\", \"isFraud\"]]\n",
        "df.to_parquet('data/train_transaction.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpAl9Jr0U5P9"
      },
      "source": [
        "### Setup AWS Infrastructure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beuzt4Pd3hwL"
      },
      "source": [
        "Since infrastructure and architecture are not the purpose of this tutorial we will use [Terraform](https://www.terraform.io) to quickly setup our infrastructure in AWS to continue with the rest of the tutorial.\n",
        "\n",
        "Without deviating too much let me just explain quickly what terraform is and the different components we set up:\n",
        "\n",
        "\n",
        "*   Terraform is a infrastructure as code tool that allows you to create and change infrastructure predictably. In plain english, think of it as a setup definition file and with one command you can create a development and production environment that are exact replicas of eachother.\n",
        "\n",
        "The following is created from the terraform file:\n",
        "\n",
        "*   **S3 bucket** - this is where we are storing our data files to be using in this tutorial\n",
        "* **Redshift cluster** - this is the AWS data warehouse we will be using\n",
        "* **AWS Glue** - this is the AWS ELT tool that we will use to get our data from S3 to redshit.\n",
        "* **AWS IAM Roles** - We create the roles thats needed for these 3 resources to interact.\n",
        "\n",
        "Okay enough geeking out on Terraform - lets code!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppqtno2O6cf6"
      },
      "source": [
        "We need to setup our AWS credentials in order to deploy this terraform setup to our account. To start make sure you have your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables setup. If not, go to your AWS console and follow the instructions below:\n",
        "\n",
        "*   Go to the IAm service\n",
        "*   Click \"*Users*\" in the sidebar\n",
        "*   Go through the steps to creat a user and attach the following policies below.\n",
        "\n",
        "If you already have a user, make sure you have the following permissions:\n",
        "\n",
        "*   AmazonRedshiftDataFullAccess\n",
        "*   AmazonS3FullAccess\n",
        "*   AWSGlueConsoleFullAccess\n",
        "*   IAMFullAccess\n",
        "\n",
        "Once a user is created, you can click on your user and go to the tab that says \"*Security Credentials*\". Scroll down and click the button that says \"Create access key\". You should then see a *Access Key* and *Secret Key* generated for you.\n",
        "\n",
        "Run the code below pasting in the generated keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q1Vo8gST6LTc"
      },
      "outputs": [],
      "source": [
        "!export AWS_ACCESS_KEY_ID=AKIAUGLYMD63DNEJJYV5\n",
        "!export AWS_SECRET_ACCESS_KEY=Lyv3R94saoacYLWG0pAkNVzUkNw3FtVcbhi4m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY4Aetp09FfV"
      },
      "source": [
        "Install the Terraform framework. We use Homebrew on macOS but you may install it however you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fdjR4ptu8v4R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running `brew update --auto-update`...\n",
            "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
            "Updated 3 taps (homebrew/core, homebrew/cask and homebrew/cask-fonts).\n",
            "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
            "create-api      ksh93           prr             pymupdf         snapcast\n",
            "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
            "v2ray-unofficial\n",
            "\n",
            "You have \u001b[1m2\u001b[0m outdated formulae installed.\n",
            "You can upgrade them with \u001b[1mbrew upgrade\u001b[0m\n",
            "or list them with \u001b[1mbrew outdated\u001b[0m.\n",
            "\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/terraform/manifests/1.2.6\u001b[0m\n",
            "######################################################################## 100.0%\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/terraform/blobs/sha256:e540a9b5\u001b[0m\n",
            "\u001b[34m==>\u001b[0m \u001b[1mDownloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh\u001b[0m\n",
            "######################################################################## 100.0%\n",
            "\u001b[34m==>\u001b[0m \u001b[1mPouring terraform--1.2.6.arm64_monterey.bottle.tar.gz\u001b[0m\n",
            "沚ｺ  /opt/homebrew/Cellar/terraform/1.2.6: 6 files, 66.2MB\n",
            "\u001b[34m==>\u001b[0m \u001b[1m`brew cleanup` has not been run in the last 30 days, running now...\u001b[0m\n",
            "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
            "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/brotli--1.0.9... (993.3KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/c-ares--1.18.1_1... (156.7KB)\n",
            "Removing: /opt/homebrew/Cellar/ca-certificates/2022-07-19... (3 files, 222.6KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/clojurescript--1.11.4... (26.8MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/gdbm--1.23... (269.2KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/gettext--0.21... (8.7MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/icu4c--70.1... (28.0MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/libssh2--1.10.0... (331.2KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/libtermkey--0.22... (60.2KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/luajit--2.1.0-beta3-20220712.6... (796.4KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/minikube--1.26.0... (29.2MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/mpdecimal--2.5.1... (540.7KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/ncurses--6.3... (2.3MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/pkg-config--0.29.2_3... (237.2KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/readline--8.1.2... (560.3KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/rlwrap--0.45.2... (131.3KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/thefuck--3.32... (2.7MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/unibilium--2.1.1... (145.9KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/xz--5.2.5... (414.8KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/libuv_bottle_manifest--1.44.1... (6.3KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/node_bottle_manifest--17.8.0... (13.4KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/libtermkey_bottle_manifest--0.22... (8.8KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/python@3.10_bottle_manifest--3.10.2... (18.7KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/brotli_bottle_manifest--1.0.9... (8.5KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/gdbm_bottle_manifest--1.23... (6.1KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/xz_bottle_manifest--5.2.5... (7.4KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/libssh2_bottle_manifest--1.10.0... (8KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/git_bottle_manifest--2.35.1... (11.2KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/cargo_cache... (16,835 files, 545.5MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/rlwrap_bottle_manifest--0.45.2... (7.8KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/openjdk_bottle_manifest--17.0.2... (10.6KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/libnghttp2_bottle_manifest--1.47.0... (6.3KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/sqlite_bottle_manifest--3.38.1... (6.9KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/python@3.9_bottle_manifest--3.9.12... (18.1KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/starship_bottle_manifest--1.5.4... (7.3KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/openjdk_bottle_manifest--18... (10.5KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/fish_bottle_manifest--3.4.1... (9.5KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/c-ares_bottle_manifest--1.18.1_1... (6.3KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/fish_bottle_manifest--3.4.0... (9.5KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/ca-certificates_bottle_manifest--2022-03-29... (1.8KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/ca-certificates_bottle_manifest--2022-03-18... (1.8KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/unibilium_bottle_manifest--2.1.1... (7.2KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/rust_bottle_manifest--1.59.0... (10.4KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/starship_bottle_manifest--1.4.2... (7.3KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/clojure_bottle_manifest--1.11.0.1100... (6.5KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/pcre2_bottle_manifest--10.39... (7.4KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/yarn_bottle_manifest--1.22.18... (1.8KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/sqlite_bottle_manifest--3.38.2... (6.9KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/python@3.9_bottle_manifest--3.9.10... (18.1KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/openssl@1.1_bottle_manifest--1.1.1n... (7.6KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/mpdecimal_bottle_manifest--2.5.1... (6.9KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/krb5_bottle_manifest--1.19.3... (10KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/ncurses_bottle_manifest--6.3... (9KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/clojure_bottle_manifest--1.10.3.1087... (2KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/gettext_bottle_manifest--0.21... (10.5KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/luv_bottle_manifest--1.43.0-0... (6.7KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/readline_bottle_manifest--8.1.2... (6.6KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/icu4c_bottle_manifest--70.1... (7.1KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/pkg-config_bottle_manifest--0.29.2_3... (7.4KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/postgresql_bottle_manifest--14.2_1... (14.3KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/thefuck_bottle_manifest--3.32... (13.6KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/luajit-openresty_bottle_manifest--2.1-20220310... (6.5KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/clojurescript_bottle_manifest--1.11.4... (1.8KB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/Cask/font-fira-code--6.2.zip... (2.3MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/Cask/oracle-jdk--18.0.1.1.dmg... (167.8MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/Cask/font-jetbrains-mono--2.242.zip... (3.9MB)\n",
            "Removing: /Users/elijahrou/Library/Caches/Homebrew/Cask/dynamodb-local--latest.tar.gz... (41.9MB)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/wget... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/libidn2... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/minikube... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/cmake... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/clojure... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/wangle... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/boost... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/fmt... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/starship... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/libunistring... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/serverless... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/rust... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/glog... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/lz4... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/fbthrift... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/nvm... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/libnghttp2... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/double-conversion... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/gflags... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/snappy... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/zstd... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/watchman... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/libsodium... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/libevent... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/pcre... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/python@3.10... (2 files, 2.6KB)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/neovim... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/fish... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/folly... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/fb303... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/git... (64B)\n",
            "Removing: /Users/elijahrou/Library/Logs/Homebrew/fizz... (64B)\n",
            "Pruned 0 symbolic links and 2 directories from /opt/homebrew\n"
          ]
        }
      ],
      "source": [
        "!brew install terraform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFmcfGRr9Tkv"
      },
      "source": [
        "In your terminal, go to the \"infra\" folder that came along with this tutorial. Run the command below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I2jjq2Dt9QTi"
      },
      "outputs": [],
      "source": [
        "#Run the following in the infra folder\n",
        "!terraform init\n",
        "!export TF_VAR_region=\"us-west-2\" #region to deploy this\n",
        "!export TF_VAR_project_name=\"fraud-classifier\" #setup a project nam!e\n",
        "!terraform apply -var=\"admin_password=thisISTestPassword1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7jUTW4eACg4"
      },
      "source": [
        "Once your infrastructure is deployed you should see the following output in your terminal. Save these, we will need them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPPqkVkqAHG5"
      },
      "outputs": [],
      "source": [
        "redshift_cluster_identifier = \"my-feast-project-aws-cerebrium-redshift-cluster\"\n",
        "redshift_spectrum_arn = \"arn:aws:iam::ACCOUNT_NUMBER:role/s3_spectrum_role\"\n",
        "credit_history_table = \"credit_history\"\n",
        "zipcode_features_table = \"zipcode_features\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gctsv-cFAz-6",
        "outputId": "b278cf64-bef3-4757-d6ce-bd9e58f62956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: aws: command not found\n"
          ]
        }
      ],
      "source": [
        "#run the following in your terminal\n",
        "aws redshift-data execute-statement \\\n",
        "--region us-west-2 \\\n",
        "--cluster-identifier [SET YOUR redshift_cluster_identifier HERE] \\\n",
        "--db-user admin \\\n",
        "--database dev \\\n",
        "--sql \"create external schema spectrum from data catalog database 'dev' iam_role '[SET YOUR redshift_spectrum_arn here]' create external database if not exists;\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9BpGU90KPsu"
      },
      "source": [
        "You should then get a JSON result back. Enter the id returned below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT6Lh3lPIZmb"
      },
      "outputs": [],
      "source": [
        "aws redshift-data describe-statement --id [SET YOUR STATEMENT ID HERE] --region us-west-2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRJsduJdKWmD"
      },
      "source": [
        "If that is all running successfully then we are done with our AWS setup! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJkgR8cRKc6y"
      },
      "source": [
        "### Feast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_guNMgbaTtp"
      },
      "source": [
        "To get started, let us install the Feast framework. Feast can be installed using pip. Run this command in your command line instead of running it in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiRQvjw0aQ9s",
        "outputId": "d3847036-75ed-4ad0-b254-97089a956682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting feast\n",
            "  Downloading feast-0.23.0.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (3.1.2)\n",
            "Collecting tenacity<9,>=7\n",
            "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting proto-plus<2,>=1.20.0\n",
            "  Downloading proto_plus-1.20.6-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m46.4/46.4 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate<1,>=0.8.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (0.8.9)\n",
            "Collecting google-api-core<3,>=1.23.0\n",
            "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m114.6/114.6 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toml<1,>=0.10.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (0.10.2)\n",
            "Collecting typeguard\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: SQLAlchemy[mypy]<2,>1 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (1.3.24)\n",
            "Collecting dask<2022.02.0,>=2021.*\n",
            "  Downloading dask-2022.1.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (3.2.0)\n",
            "Collecting dill==0.3.*\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m95.8/95.8 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<7,>=5.4.* in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (6.0)\n",
            "Collecting grpcio<2,>=1.47.0\n",
            "  Using cached grpcio-1.47.0-cp39-cp39-macosx_11_0_arm64.whl\n",
            "Requirement already satisfied: fastapi<1,>=0.68.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (0.79.0)\n",
            "Requirement already satisfied: pyarrow<9,>=4 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (6.0.1)\n",
            "Collecting pandavro==1.5.*\n",
            "  Downloading pandavro-1.5.2.tar.gz (3.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: pygments<3,>=2.12.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (2.12.0)\n",
            "Requirement already satisfied: uvicorn[standard]<1,>=0.14.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (0.18.2)\n",
            "Collecting tensorflow-metadata<2.0.0,>=1.0.0\n",
            "  Downloading tensorflow_metadata-1.9.0-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m51.0/51.0 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.0.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (8.1.3)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (1.9.1)\n",
            "Collecting fastavro<2,>=1.1.0\n",
            "  Downloading fastavro-1.5.4.tar.gz (772 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m772.9/772.9 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pandas<2,>=1.4.3\n",
            "  Using cached pandas-1.4.3-cp39-cp39-macosx_11_0_arm64.whl (10.5 MB)\n",
            "Collecting bowler\n",
            "  Downloading bowler-0.9.0-py3-none-any.whl (36 kB)\n",
            "Collecting googleapis-common-protos<2,>=1.52.*\n",
            "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m211.7/211.7 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-reflection<2,>=1.47.0\n",
            "  Downloading grpcio_reflection-1.47.0-py3-none-any.whl (16 kB)\n",
            "Collecting protobuf<4,>3.20\n",
            "  Using cached protobuf-3.20.1-py2.py3-none-any.whl (162 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (4.64.0)\n",
            "Collecting mmh3\n",
            "  Downloading mmh3-3.0.0-cp39-cp39-macosx_11_0_arm64.whl (13 kB)\n",
            "Collecting numpy<3,>=1.22\n",
            "  Using cached numpy-1.23.1-cp39-cp39-macosx_11_0_arm64.whl (13.3 MB)\n",
            "Requirement already satisfied: colorama<1,>=0.3.9 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from feast) (0.4.4)\n",
            "Requirement already satisfied: six>=1.9 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from pandavro==1.5.*->feast) (1.16.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from dask<2022.02.0,>=2021.*->feast) (0.11.2)\n",
            "Requirement already satisfied: partd>=0.3.10 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from dask<2022.02.0,>=2021.*->feast) (1.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from dask<2022.02.0,>=2021.*->feast) (2.0.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from dask<2022.02.0,>=2021.*->feast) (2022.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from dask<2022.02.0,>=2021.*->feast) (20.9)\n",
            "Requirement already satisfied: starlette==0.19.1 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from fastapi<1,>=0.68.0->feast) (0.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from starlette==0.19.1->fastapi<1,>=0.68.0->feast) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from starlette==0.19.1->fastapi<1,>=0.68.0->feast) (4.3.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from google-api-core<3,>=1.23.0->feast) (2.28.1)\n",
            "Collecting google-auth<3.0dev,>=1.25.0\n",
            "  Downloading google_auth-2.9.1-py2.py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m167.8/167.8 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from Jinja2<4,>=2->feast) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from pandas<2,>=1.4.3->feast) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from pandas<2,>=1.4.3->feast) (2.8.1)\n",
            "\u001b[33mWARNING: sqlalchemy 1.3.24 does not provide the extra 'mypy'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting absl-py<2.0.0,>=0.9\n",
            "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m123.4/123.4 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11>=0.8 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from uvicorn[standard]<1,>=0.14.0->feast) (0.13.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from uvicorn[standard]<1,>=0.14.0->feast) (0.20.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
            "  Downloading uvloop-0.16.0-cp39-cp39-macosx_10_9_universal2.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0\n",
            "  Downloading websockets-10.3-cp39-cp39-macosx_11_0_arm64.whl (97 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m97.2/97.2 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13\n",
            "  Downloading watchfiles-0.16.1-cp37-abi3-macosx_11_0_arm64.whl (359 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m359.4/359.4 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.4.0\n",
            "  Downloading httptools-0.4.0-cp39-cp39-macosx_10_9_universal2.whl (227 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m228.0/228.0 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting volatile\n",
            "  Downloading volatile-2.1.0.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting fissix\n",
            "  Downloading fissix-21.11.13-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m188.3/188.3 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting moreorless>=0.2.0\n",
            "  Downloading moreorless-0.4.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: attrs in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from bowler->feast) (21.4.0)\n",
            "Requirement already satisfied: setuptools in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from jsonschema->feast) (62.1.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from jsonschema->feast) (0.18.1)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from packaging>=20.0->dask<2022.02.0,>=2021.*->feast) (3.0.8)\n",
            "Requirement already satisfied: locket in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from partd>=0.3.10->dask<2022.02.0,>=2021.*->feast) (1.0.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3,>=1.23.0->feast) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3,>=1.23.0->feast) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3,>=1.23.0->feast) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3,>=1.23.0->feast) (1.25.11)\n",
            "Requirement already satisfied: appdirs in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from fissix->bowler->feast) (1.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/elijahrou/mambaforge/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi<1,>=0.68.0->feast) (1.2.0)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "Building wheels for collected packages: feast, pandavro, fastavro, volatile\n",
            "  Building wheel for feast (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for feast: filename=feast-0.23.0-cp39-cp39-macosx_11_0_arm64.whl size=4244456 sha256=d52e0dee4a363f8152815f6d926cecb5f30a812ebfef5679468e690b9aa6a5c7\n",
            "  Stored in directory: /Users/elijahrou/Library/Caches/pip/wheels/bd/7f/32/87dbed273c8ffb18e09303a20229ceb385bf181e70b63753cb\n",
            "  Building wheel for pandavro (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pandavro: filename=pandavro-1.5.2-py3-none-any.whl size=2953 sha256=a9b16d089839eb699e9067c8362e5883c0a733e484c77a5369b26de4563c897d\n",
            "  Stored in directory: /Users/elijahrou/Library/Caches/pip/wheels/bc/da/0d/f0467cb9bb00c50da43572b76fcab102402291783f5e2e09cb\n",
            "  Building wheel for fastavro (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for fastavro: filename=fastavro-1.5.4-cp39-cp39-macosx_11_0_arm64.whl size=442448 sha256=10a37964813307dd9c34e6978128fee5b48fdc9b192ec1017836f662a152ee96\n",
            "  Stored in directory: /Users/elijahrou/Library/Caches/pip/wheels/9c/b3/e7/852ab8971d280943b3a0598ad97c981864820a8c92a51d6633\n",
            "  Building wheel for volatile (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for volatile: filename=volatile-2.1.0-py3-none-any.whl size=3468 sha256=84c07b30618a8dbdacf975d4c72a20400a90dc470037fa2d224a145c5536cd03\n",
            "  Stored in directory: /Users/elijahrou/Library/Caches/pip/wheels/0c/b1/b9/9b0cfdf9f8ac4da73948288509d06e0d4a8c4589746d9fa44c\n",
            "Successfully built feast pandavro fastavro volatile\n",
            "Installing collected packages: volatile, pyasn1, mmh3, websockets, uvloop, typeguard, tenacity, rsa, pyasn1-modules, protobuf, numpy, moreorless, httptools, grpcio, fissix, fastavro, dill, cachetools, absl-py, watchfiles, proto-plus, pandas, grpcio-reflection, googleapis-common-protos, google-auth, dask, bowler, tensorflow-metadata, pandavro, google-api-core, feast\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 9.1\n",
            "    Uninstalling websockets-9.1:\n",
            "      Successfully uninstalled websockets-9.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.4\n",
            "    Uninstalling protobuf-3.19.4:\n",
            "      Successfully uninstalled protobuf-3.19.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.45.0\n",
            "    Uninstalling grpcio-1.45.0:\n",
            "      Successfully uninstalled grpcio-1.45.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.1\n",
            "    Uninstalling pandas-1.4.1:\n",
            "      Successfully uninstalled pandas-1.4.1\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2022.3.0\n",
            "    Uninstalling dask-2022.3.0:\n",
            "      Successfully uninstalled dask-2022.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "statsmodels 0.13.2 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "shap 0.40.0 requires packaging>20.9, but you have packaging 20.9 which is incompatible.\n",
            "prefect 1.2.0 requires urllib3>=1.26.0, but you have urllib3 1.25.11 which is incompatible.\n",
            "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.23.1 which is incompatible.\n",
            "lightning-grid 0.8.32 requires PyYAML<6.0,>=5.0, but you have pyyaml 6.0 which is incompatible.\n",
            "lightning-grid 0.8.32 requires urllib3<2.0.0,>=1.26, but you have urllib3 1.25.11 which is incompatible.\n",
            "lightning-grid 0.8.32 requires websocket-client==1.2.1, but you have websocket-client 1.3.2 which is incompatible.\n",
            "lightning-grid 0.8.32 requires websockets<10.0,>=9.0, but you have websockets 10.3 which is incompatible.\n",
            "flytekit 0.26.1 requires click<8.0,>=6.6, but you have click 8.1.3 which is incompatible.\n",
            "distributed 2022.3.0 requires dask==2022.03.0, but you have dask 2022.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-1.2.0 bowler-0.9.0 cachetools-5.2.0 dask-2022.1.1 dill-0.3.5.1 fastavro-1.5.4 feast-0.23.0 fissix-21.11.13 google-api-core-2.8.2 google-auth-2.9.1 googleapis-common-protos-1.56.4 grpcio-1.47.0 grpcio-reflection-1.47.0 httptools-0.4.0 mmh3-3.0.0 moreorless-0.4.0 numpy-1.23.1 pandas-1.4.3 pandavro-1.5.2 proto-plus-1.20.6 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.9 tenacity-8.0.1 tensorflow-metadata-1.9.0 typeguard-2.13.3 uvloop-0.16.0 volatile-2.1.0 watchfiles-0.16.1 websockets-10.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install feast\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVLpSnxbah5N"
      },
      "source": [
        "In Feast, you define your features using a .yaml file in a repository. To create a repository run the command below. This will create a few files that are mostly example files (you can delete driver_repo.py and test.py if you would like) but we only care about:\n",
        "\n",
        "*   **example.py**: This is a python file where you define your feature values and where Feast will find them. i.e: A Redshift Cluster or a S3 bucket.\n",
        "*   **feature_store.yaml**: This is a configuration file where you will define the location of your Redshift cluster, S3 bucket and DynamoDB Database.\n",
        "\n",
        "\n",
        "Since we are using AWS, we have use *aws* in the command however if you were using Google Cloud you can use *gcp*. Run the commands below in your terminal, if you run it in this notebook it will create the folders in here. Create a folder and change to it in your command line and run the command below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--Blu1xObLoE",
        "outputId": "940b1daa-3ebd-4233-92e1-c5d41cbd6d9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AWS Region (e.g. us-west-2): \n",
            "AWS Region (e.g. us-west-2): \n",
            "AWS Region (e.g. us-west-2): us-west-2\n",
            "Redshift Cluster ID: my-feast-project-aws-cerebrium-redshift-cluster\n",
            "Redshift Database Name: dev\n",
            "Redshift User Name: admin\n",
            "Redshift S3 Staging Location (s3://*): \n",
            "Aborted!\n"
          ]
        }
      ],
      "source": [
        "!feast init -t aws feature_repo # Command only shown for reference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNhC9VF-kFaY"
      },
      "source": [
        "Create a file called example.py in which we will define our features. Before we get started, we need to understand the concept of an Entity and FeatureView:\n",
        "\n",
        "*   **Entity**: An entity is a collection of semantically related features. For example, Uber would have customers and drivers as two seperate entities that group features that correspond to those entities.\n",
        "*   **FeatureView**: A feature view is an object that represents a logical group of time-series feature data as it is found in a data source. They consist of zero or more entities, one or more features and a data source.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccj4iVfRmBvb"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "from feast import (Entity, Feature, FeatureView, RedshiftSource,\n",
        "                   ValueType)\n",
        "\n",
        "transaction = Entity(name=\"transactions\", value_type=ValueType.STRING)\n",
        "\n",
        "transaction_source = RedshiftSource(\n",
        "    query=\"SELECT * FROM spectrum.transaction_features\",\n",
        "    event_timestamp_column=\"event_timestamp\",\n",
        "    created_timestamp_column=\"created_timestamp\",\n",
        ")\n",
        "\n",
        "transaction_features = FeatureView(\n",
        "    name=\"transaction_features\",\n",
        "    entities=[\"transaction\"],\n",
        "    ttl=timedelta(days=365),\n",
        "    features=[\n",
        "        Feature(name=\"ProductCD\", dtype=ValueType.STRING),\n",
        "        Feature(name=\"P_emaildomain\", dtype=ValueType.STRING),\n",
        "        Feature(name=\"R_emaildomain\", dtype=ValueType.STRING),\n",
        "        Feature(name=\"card4\", dtype=ValueType.STRING),\n",
        "        Feature(name=\"M1\", dtype=ValueType.STRING),\n",
        "        Feature(name=\"M2\", dtype=ValueType.STRING),\n",
        "        Feature(name=\"M3\", dtype=ValueType.STRING)\n",
        "    ],\n",
        "    batch_source=transaction_source,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OSRJtZ_rSyM"
      },
      "source": [
        "First we create our *transaction* entity and define the SQL that will fetch the required features from our Redshift data warehouse. We then create a featureView that uses the Redshift instance to fetch the features and define the data type for each feature. We also define the time we would like the feature to contain. In this case we want 1 year worth of data which is 365 days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilJ6kYA8bkti"
      },
      "source": [
        "Next, we'll edit the `feature_store.yaml` file to reference our Redshift cluster and s3 bucket - these are the values you got returned when terraform finished running. Below is what the various fields mean:\n",
        "\n",
        "* **project**: The name you would like to call the project.\n",
        "* **registry**: The registry is a central catalog of all the feature definitions and their related metadata. It is a file that you can interact with through the Feast API\n",
        "*   **provider**: The cloud provider you are using - in our case AWS\n",
        "*   **online_store**: The Online store is used for low-latency online feature value lookups. Feature values are loaded into the online store from data sources. Online stores only hold the latest values per entity key. An online store would be something such as Redis or DynamoDB - low latency.\n",
        "* **offline_store**: The offline stores store historic feature values and does not generate these values. The offline store is used as the interface for querying existing features or loading these features into an online store for low latency prediction. An offline store would be something like a data warehouse or storage bucket - high latency and a alot of historical data.\n",
        "\n",
        "\n",
        "Your S3 bucket name you fill in below should be something along the lines of s3://my-feast-project-aws-cerebrium-bucket where cerebrium should be replaced with your project name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyZ_iem0duNJ"
      },
      "outputs": [],
      "source": [
        "project: credit_scoring_aws\n",
        "registry: registry.db\n",
        "provider: aws\n",
        "online_store:\n",
        "    type: dynamodb\n",
        "    region: us-west-2\n",
        "offline_store:\n",
        "    type: redshift\n",
        "    cluster_id: [SET YOUR CLUSTER ID]\n",
        "    region: us-west-2\n",
        "    database: dev\n",
        "    user: admin\n",
        "    s3_staging_location: s3://[SET YOUR BUCKET NAME]\n",
        "    iam_role: [SET YOUR ARN]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG5XPr__dyKM"
      },
      "source": [
        "Deploy the feature store by running apply from within the feature/ folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7QrUSUTb3o6"
      },
      "outputs": [],
      "source": [
        "!feast apply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxF7s9oB-EH"
      },
      "source": [
        "If everything was created correctly, you would have seen the following output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beodYs1PCDEr"
      },
      "outputs": [],
      "source": [
        "#Created entity zipcode\n",
        "#Created entity dob_ssn\n",
        "#Created feature view credit_history\n",
        "#Created feature view zipcode_features\n",
        "\n",
        "#Deploying infrastructure for credit_history\n",
        "#Deploying infrastructure for zipcode_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALWd7-r7cj6n"
      },
      "source": [
        "Next we load our features into the online store using the materialize-incremental command. This command will load the latest feature values from a data source into the online store from the last materialize call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww0kqHnru566"
      },
      "outputs": [],
      "source": [
        "CURRENT_TIME=$(date -u +\"%Y-%m-%dT%H:%M:%S\")\n",
        "feast materialize-incremental $CURRENT_TIME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM2yRozawF4p"
      },
      "source": [
        "If successful, you should see some activity in your terminal that its uploading the features. Once completed, you should see the results in our DynamoDB instance on AWS. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuPaZtS0wQut"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzzOaPhxwUYl"
      },
      "source": [
        "In the repo, we have two files with respect to our model:\n",
        "\n",
        "*   *run.py*: This is a helper file that is going through the full model workflow. It fetches the historical loan data, trains our model and then makes a prediction to determine if the loan was approved or not.\n",
        "*   *credit_model.py*: This file shows you how we use Feast during our model building as well as during our inference. \n",
        "\n",
        "We will go through the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZOcZJaOu8OD"
      },
      "outputs": [],
      "source": [
        "#run.py\n",
        "import boto3\n",
        "import pandas as pd\n",
        "\n",
        "#Import Class that contains most of the functionality\n",
        "from fraud_detection_model import FraudClassifierModel\n",
        "\n",
        "# Get historic transaction data\n",
        "transactions = pd.read_parquet(\"data/train_transaction.parquet\")\n",
        "\n",
        "# Create model\n",
        "model = FraudClassifierModel\n",
        "\n",
        "# Train model (using Redshift for zipcode and credit history features)\n",
        "if not model.is_model_trained():\n",
        "    model.train(transactions)\n",
        "\n",
        "# Make online prediction (using DynamoDB for retrieving online features)\n",
        "transaction_request = {\n",
        "    \"ProductCD\": [\"W\"],\n",
        "    \"P_emaildomain\": [\"live\"],\n",
        "    \"R_emaildomain\": [None],\n",
        "    \"card4\": [\"visa\"],\n",
        "    \"M1\": [\"T\"],\n",
        "    \"M2\": [\"T\"],\n",
        "    \"M3\": [\"T\"]\n",
        "}\n",
        "\n",
        "result = model.predict(transaction_request)\n",
        "\n",
        "if result == 0:\n",
        "    print(\"Non-Fraudeluent Transaction\")\n",
        "elif result == 1:\n",
        "    print(\"Fradulent Transaction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr_5uGgY0QKK"
      },
      "source": [
        "In the credit_model.py we won't go through the entire file but rather just snippets in the file. \n",
        "\n",
        "We start by defining our model features which we do by specifiying the [entity name]: [column name]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGlRuHcm0l7c"
      },
      "outputs": [],
      "source": [
        "#line 21\n",
        "feast_features = [\n",
        "        \"transaction_features:ProductCD\",\n",
        "        \"transaction_features:isFraud\",\n",
        "        \"transaction_features:P_emaildomain\",\n",
        "        \"transaction_features:R_emaildomain\",\n",
        "        \"transaction_features:card4\",\n",
        "        \"transaction_features:M1\",\n",
        "        \"transaction_features:M2\",\n",
        "        \"transaction_features:M3\"\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqfhg1cI08LW"
      },
      "source": [
        "During the initialisation of our model we attach the feature store to our model object to use later. The repo path is where the folder that contains our feature_store.yaml and example.py that we created above - Feast fetches the configuration from there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj4_9b-v1BhA"
      },
      "outputs": [],
      "source": [
        "#57\n",
        "self.fs = feast.FeatureStore(repo_path=\"feature_repo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRpqFpJC1gtW"
      },
      "source": [
        "When we would like to train our model, we want to get the historical data relating to our features. The method below launches a job that executes a join of features from the offline store onto the entity dataframe. \n",
        "\n",
        "An entity dataframe is the target dataframe on which you would like to join feature values. The entity dataframe must contain a timestamp column called event_timestamp and all entities (primary keys) necessary to join feature tables onto. All entities found in feature views that are being joined onto the entity dataframe must be found as column on the entity dataframe.\n",
        "\n",
        "Once completed, a job reference will be returned. This job reference can then be converted to a Pandas dataframe by calling to_df()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juEk054b1hzY"
      },
      "outputs": [],
      "source": [
        "#line 66\n",
        "training_df = self.fs.get_historical_features(\n",
        "            entity_df=loans, features=self.feast_features\n",
        "        ).to_df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOzPsPbP4ZWW"
      },
      "source": [
        "When we do online inference (prediction) using our model, we don't want to have to fetch all the historical data or anything really from our data warehouse since that will take multiple seconds. Rather we want to get the data we need from a low-latency data-source so we can have a low response time (~100ms). We do that below with the get_online_features function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng3ALAeR4YYv"
      },
      "outputs": [],
      "source": [
        "#line 123\n",
        "return self.fs.get_online_features(\n",
        "    entity_rows=[{\"transaction\": transaction}],\n",
        "    features=self.feast_features,\n",
        ").to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATIpmj7S8Sqo"
      },
      "source": [
        "The above allows me to pass in the specific transaction and get the feature values for this user instantaneously. We can then use these values in our predict *function* to return what we predicted for the loan\n",
        "\n",
        "Now let us run our run.py file to see this live and the output of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cMhhiu9x5F9",
        "outputId": "5f39428b-7554-408f-872d-291e24191563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file 'run.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "python run.py\n",
        "\n",
        "#Output\n",
        "\n",
        "#loan rejected!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXYwtNBnNVu8"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50zf2nDqNZZT"
      },
      "source": [
        "That's it for our tutorial on feature stores! As I am sure you can tell, feature stores can add a lot of value to your ML infrastructure when it comes to using the same features across multiple models as well as doing server-side feature calculations however can add some complxity. Using Feast is great to implement this but if you want a more managed approach with extra functionality such as identifying model drift then you can try Tecton, or the the features stores that are native to the AWS and Google platforms."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "D1dwtH_4Qbe5",
        "qzOSuB-P-rz6",
        "tbjxAykZwveS",
        "VXb8YqI0JsUN",
        "XpAl9Jr0U5P9",
        "HJkgR8cRKc6y",
        "XXYwtNBnNVu8"
      ],
      "name": "Feature Store.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "8f7fc29b09bceb45b96f31c421af977920dd07416ec6e051f1e8d585e72b0202"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
